from tqdm import tqdm
import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from kan import KAN
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (roc_auc_score, matthews_corrcoef,
                             f1_score, accuracy_score)
import re

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
clinical_data_path = "C:/Users/User/Desktop/IVF/AI/Blastocyst_Dataset/Clincial_annotations.xlsx"
dino_features_path = "dino_features.pt"  # –§–∞–π–ª —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏ DINO
batch_size = 32
input_size = 1024 + 7  # –†–∞–∑–º–µ—Ä DINO-—Ñ–∏—á + 7 –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
clinical_data = pd.read_excel(clinical_data_path)
clinical_features = [
    'EXP_silver', 'ICM_silver', 'TE_silver',
    'COC', 'MII', 'Age', 'Endo'
]

def load_and_process_data(clinical_data_path, dino_features_path, clinical_features):
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    clinical_data = pd.read_excel(clinical_data_path)

    # –ó–∞–≥—Ä—É–∑–∫–∞ DINO-—Ñ–∏—á
    dino_data = torch.load(dino_features_path)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã DINO-—Ñ–∏—á
    if isinstance(dino_data, dict):
        assert 'filenames' in dino_data and 'features' in dino_data, "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ DINO-—Ñ–∏—á!"
        filenames = dino_data['filenames']
        features = dino_data['features']
    else:
        raise ValueError("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç DINO-—Ñ–∏—á!")

    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è DINO-—Ñ–∏—á
    dino_df = pd.DataFrame({
        'Image': filenames,
        'dino_features': [f.numpy() if isinstance(f, torch.Tensor) else f for f in features]
    })

    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤
    def clean_image_name(name):
        name = str(name).lower().strip()
        name = re.sub(r'\.png$', '', name)
        return re.split(r'[/\\]', name)[-1]  # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞

    clinical_data['Image'] = clinical_data['Image'].apply(clean_image_name)
    dino_df['Image'] = dino_df['Image'].apply(clean_image_name)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    print("\n–ü—Ä–∏–º–µ—Ä –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –∏–º–µ–Ω –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:", clinical_data['Image'].head().values)
    print("–ü—Ä–∏–º–µ—Ä DINO-–∏–º–µ–Ω –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:", dino_df['Image'].head().values)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    full_data = clinical_data.merge(
        dino_df,
        on='Image',
        how='inner'
    )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if full_data.empty:
        sample_clin = clinical_data['Image'].head(3).tolist()
        sample_dino = dino_df['Image'].head(3).tolist()
        raise ValueError(
            f"–ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è!\n"
            f"–ü—Ä–∏–º–µ—Ä –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –∏–º–µ–Ω: {sample_clin}\n"
            f"–ü—Ä–∏–º–µ—Ä DINO-–∏–º–µ–Ω: {sample_dino}"
        )

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    scaler = StandardScaler()
    full_data.loc[:, clinical_features] = scaler.fit_transform(full_data[clinical_features])

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    full_data = full_data.dropna(subset=clinical_features + ['HA'])

    print("\n–£—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(full_data)}")
    print(f"–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:\n{full_data[['Image', 'HA'] + clinical_features].head(3)}")

    return full_data, scaler

class CombinedDataset(Dataset):
    def __init__(self, data):
        self.data = data.reset_index(drop=True)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]

        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ DINO-—Ñ–∏—á
            dino_feats = row['dino_features']
            if not isinstance(dino_feats, np.ndarray):
                dino_feats = np.array(dino_feats)

            dino_tensor = torch.tensor(dino_feats, dtype=torch.float32)

            # –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏
            clinical_tensor = torch.tensor(
                row[clinical_features].values.astype(np.float32),
                dtype=torch.float32
            )

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
            if dino_tensor.ndim == 0:
                dino_tensor = dino_tensor.unsqueeze(0)

            combined = torch.cat([dino_tensor.flatten(), clinical_tensor])

            target = torch.tensor(row['HA'], dtype=torch.float32)

            return combined, target

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {row['Image']}: {str(e)}")
            raise

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
try:
    full_data, scaler = load_and_process_data(
        clinical_data_path="C:/Users/User/Desktop/IVF/AI/Blastocyst_Dataset/Clincial_annotations.xlsx",
        dino_features_path="dino_features.pt",
        clinical_features=['EXP_silver', 'ICM_silver', 'TE_silver', 'COC', 'MII', 'Age', 'Endo']
    )

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    train_df, test_df = train_test_split(
        full_data,
        test_size=0.2,
        random_state=42,
        stratify=full_data['HA']
    )

    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
    train_loader = DataLoader(
        CombinedDataset(train_df),
        batch_size=32,
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )

    test_loader = DataLoader(
        CombinedDataset(test_df),
        batch_size=32,
        num_workers=2,
        pin_memory=True
    )

except Exception as e:
    print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
train_df, test_df = train_test_split(
    full_data,
    test_size=0.2,
    random_state=42,
    stratify=full_data['HA']
)

# –°–æ–∑–¥–∞–Ω–∏–µ DataLoader —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –±–∞—Ç—á–µ–π
train_loader = DataLoader(
    CombinedDataset(train_df),
    batch_size=batch_size,
    shuffle=True,
    drop_last=True
)

test_loader = DataLoader(
    CombinedDataset(test_df),
    batch_size=batch_size,
    drop_last=False
)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞
for batch in train_loader:
    features, targets = batch
    print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π:")
    print("–§–æ—Ä–º–∞ —Ñ–∏—á–µ–π:", features.shape)
    print("–§–æ—Ä–º–∞ —Ü–µ–ª–µ–π:", targets.shape)
    break



# –ú–æ–¥–µ–ª—å KAN —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
class PregnancyPredictor(torch.nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.kan = KAN(width=[input_size, 10, 5, 1], grid=5, k=3)
        self.train_metrics = {'loss': []}
        self.val_metrics = {'roc_auc': [], 'mcc': [], 'f1': [], 'accuracy': [], 'loss': []}

    def forward(self, x):
        return self.kan(x).squeeze()

model = PregnancyPredictor(input_size).to(device)

# –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
criterion = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)

best_metrics = {'roc_auc': 0}
epochs = 100

with tqdm(total=epochs, desc="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", unit="epoch",
          bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as pbar_epoch:

    for epoch in range(epochs):
        # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
        model.train()
        train_loss = 0
        with tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]",
                  leave=False, unit="batch") as pbar_batch:

            for features, targets in pbar_batch:
                features, targets = features.to(device), targets.to(device)

                optimizer.zero_grad()
                outputs = model(features)
                loss = criterion(outputs, targets)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()
                pbar_batch.set_postfix(loss=f"{loss.item():.4f}")

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        all_probs = []
        all_targets = []
        val_loss = 0

        with torch.no_grad(), tqdm(test_loader, desc=f"Epoch {epoch+1}/{epochs} [Val]",
                                   leave=False, unit="batch") as pbar_val:

            for features, targets in pbar_val:
                features = features.to(device)
                outputs = model(features)
                loss = criterion(outputs, targets.to(device))

                probs = torch.sigmoid(outputs).cpu().numpy()
                all_probs.extend(probs)
                all_targets.extend(targets.cpu().numpy())
                val_loss += loss.item()

                pbar_val.set_postfix(val_loss=f"{loss.item():.4f}")

        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        roc_auc = roc_auc_score(all_targets, all_probs)
        predictions = (np.array(all_probs) > 0.5).astype(int)

        metrics = {
            'roc_auc': roc_auc,
            'mcc': matthews_corrcoef(all_targets, predictions),
            'f1': f1_score(all_targets, predictions),
            'accuracy': accuracy_score(all_targets, predictions),
            'loss': val_loss / len(test_loader)
        }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if metrics['roc_auc'] > best_metrics['roc_auc']:
            best_metrics = metrics.copy()
            torch.save(model.state_dict(), 'best_model.pth')
            pbar_epoch.write(f"üöÄ –ù–æ–≤—ã–π —Ä–µ–∫–æ—Ä–¥ ROC AUC: {metrics['roc_auc']:.4f}")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        pbar_epoch.set_postfix({
            'Train Loss': f"{train_loss/len(train_loader):.4f}",
            'Val Loss': f"{metrics['loss']:.4f}",
            'ROC AUC': f"{metrics['roc_auc']:.4f}",
            'F1': f"{metrics['f1']:.4f}"
        })

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
        if (epoch+1) % 5 == 0:
            pbar_epoch.write("\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏ {}:".format(epoch+1))
            pbar_epoch.write("‚îÇ Train Loss: {:.4f}".format(train_loss/len(train_loader)))
            pbar_epoch.write("‚îÇ Val Loss:    {:.4f}".format(metrics['loss']))
            pbar_epoch.write("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
            pbar_epoch.write("‚îÇ ROC AUC:     {:.4f}".format(metrics['roc_auc']))
            pbar_epoch.write("‚îÇ MCC:         {:.4f}".format(metrics['mcc']))
            pbar_epoch.write("‚îÇ F1:          {:.4f}".format(metrics['f1']))
            pbar_epoch.write("‚îî‚îÄAccuracy:    {:.4f}\n".format(metrics['accuracy']))

        pbar_epoch.update(1)

# –§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
print("\nüèÜ –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
print("‚îú ROC AUC:     {:.4f}".format(best_metrics['roc_auc']))
print("‚îú MCC:         {:.4f}".format(best_metrics['mcc']))
print("‚îú F1:          {:.4f}".format(best_metrics['f1']))
print("‚îî Accuracy:    {:.4f}".format(best_metrics['accuracy']))
